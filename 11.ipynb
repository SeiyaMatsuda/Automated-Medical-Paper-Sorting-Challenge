{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24625a1-7ba1-47b4-b1ab-211d6459c363",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "1. データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed6cdea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.optimize import minimize, minimize_scalar\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import transformers as T\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23e20c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74829744-95a2-4e2a-b2ec-5401640d6b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ab13eb-6570-472c-97ee-eab8f5821e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 471\n",
    "seed_torch(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8519c10e-d167-4886-a9c8-8d089db471f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './dataset/data4'\n",
    "OUTPUT_DIR = './result/result10/'\n",
    "if not os.path.isdir(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13dd61c3-7492-47af-b121-084f344fcdc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_logger(log_file=OUTPUT_DIR + \"/train.log\"):\n",
    "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db7afb3d-e9f1-4e87-a78d-488bfa71db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR  +\"/train.csv\", index_col=0)\n",
    "test = pd.read_csv(DATA_DIR + \"/test.csv\", index_col=0)\n",
    "sub = pd.read_csv(DATA_DIR + \"/sample_submit.csv\", header=None)\n",
    "sub.columns = [\"id\", \"judgement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89139f30-1f20-4989-9616-e7fbdf0380b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_fbeta_threshold(y_true, y_pred):\n",
    "    \"\"\"fbeta score計算時のthresholdを最適化\"\"\"\n",
    "    def opt_(x): \n",
    "        return -fbeta_score(y_true, y_pred >= x, beta=7)\n",
    "    result = minimize(opt_, x0=np.array([0.1]), method='Powell')\n",
    "    best_threshold = result['x'].item()\n",
    "    return best_threshold\n",
    "\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    \"\"\"fbeta(beta=7)の閾値最適化評価関数\"\"\"\n",
    "    bt = opt_fbeta_threshold(y_true, y_pred)\n",
    "    print(f\"bt:{bt}\")\n",
    "    score = fbeta_score(y_true, y_pred >= bt, beta=7)\n",
    "    return score, bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92dad8e7-a220-4b5f-ab7d-6dd5f11345e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, df, model_name, include_labels=True):\n",
    "        tokenizer = T.BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.df = df\n",
    "        self.include_labels = include_labels\n",
    "\n",
    "        self.text = df[\"text\"]\n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',            \n",
    "            max_length = 500,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "\n",
    "        \n",
    "        if self.include_labels:\n",
    "            self.labels = df[\"judgement\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][idx])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][idx])\n",
    "\n",
    "        if self.include_labels:\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "            return input_ids, attention_mask, label\n",
    "\n",
    "        return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d20a5d76-0d5c-4255-ab0c-aa0a0fd7be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = T.BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        out = self.sigmoid(out.logits).squeeze()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6e7c113-06d2-4ab1-853c-37eec7a0f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.weight = torch.Tensor([alpha, 1-alpha])\n",
    "        self.nllLoss = nn.NLLLoss(weight=self.weight).to('cuda')\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input = input.unsqueeze(1)\n",
    "        softmax = torch.cat([1-input, input], dim=1)\n",
    "        log_logits = torch.log(softmax)\n",
    "        target = target.long()\n",
    "        fix_weights = (1 - softmax) ** self.gamma\n",
    "        logits = fix_weights * log_logits\n",
    "        logits = logits\n",
    "        target = target\n",
    "        return self.nllLoss(logits, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "464b0034-7552-4b78-8411-7b718c15e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_sampling(data: pd.DataFrame):\n",
    "\n",
    "    #order_of_finish = 着順\n",
    "    low_frequency_data_sample = data[data[\"judgement\"] == 1]\n",
    "    low_frequency_data_size = len(low_frequency_data_sample)\n",
    "    # 高頻度データの行ラベル\n",
    "    high_frequency_data = data[data[\"judgement\"] == 0].index\n",
    "\n",
    "    # 高頻度データの行ラベルから、低頻度のデータと同じ数をランダムで抽出\n",
    "    random_indices = np.random.choice(high_frequency_data, low_frequency_data_size, replace=False)\n",
    "\n",
    "    # 抽出した行ラベルを使って、該当するデータを取得\n",
    "    high_frequency_data_sample = data.loc[random_indices]\n",
    "    pd.DataFrame(high_frequency_data_sample)\n",
    "\n",
    "    # データをマージする。 concatは結合API\n",
    "    merged_data = pd.concat([high_frequency_data_sample, low_frequency_data_sample], ignore_index=True)\n",
    "    balanced_data = pd.DataFrame(merged_data)\n",
    "\n",
    "    return balanced_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cceceab6-1098-4296-b9f7-89a9b3b2fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b66bbde8-0a88-4bc7-9aab-1e3e2d87be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    start = end = time.time()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        y_preds = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0 or step == (len(train_loader) - 1):\n",
    "            print(\n",
    "                f\"Epoch: [{epoch + 1}][{step}/{len(train_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(train_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "            )\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3eb1b26-4538-4a05-b4d6-16165bdc49c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    start = end = time.time()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(valid_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # record score\n",
    "        preds.append(y_preds.to(\"cpu\").numpy())\n",
    "\n",
    "        if step % 100 == 0 or step == (len(valid_loader) - 1):\n",
    "            print(\n",
    "                f\"EVAL: [{step}/{len(valid_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(valid_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "            )\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dc9ca74-bc5f-4fb7-89b8-3d287df9dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train, fold):\n",
    "\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # Data Loader\n",
    "    # ====================================================\n",
    "    trn_idx = train[train[\"fold\"] != fold].index\n",
    "    val_idx = train[train[\"fold\"] == fold].index\n",
    "    \n",
    "    train_folds = train.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = train.loc[val_idx].reset_index(drop=True)\n",
    "#     train_folds = under_sampling(train_folds)\n",
    "    train_dataset = BaseDataset(train_folds, \"bert-base-uncased\")\n",
    "    valid_dataset = BaseDataset(valid_folds, \"bert-base-uncased\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # Model\n",
    "    # ====================================================\n",
    "    model = BaseModel(\"bert-base-uncased\")\n",
    "    model.to(device)\n",
    "    if torch.cuda.device_count()>=2:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = T.AdamW(model.parameters(), lr=2e-5, weight_decay = 1e-5)\n",
    "\n",
    "    criterion = FocalLoss()\n",
    "    # ====================================================\n",
    "    # Loop\n",
    "    # ====================================================\n",
    "    best_score = -1\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(10):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
    "        valid_labels = valid_folds[\"judgement\"].values\n",
    "\n",
    "        # scoring\n",
    "        score, border = metrics(valid_labels, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        LOGGER.info(\n",
    "            f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\"\n",
    "        )\n",
    "        LOGGER.info(f\"Epoch {epoch+1} - Score: {score} -Border: {border}\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_border =border\n",
    "            LOGGER.info(f\"Epoch {epoch+1} - Save Best Score: {best_score:.4f} - Save Best Score: {best_border:.4f} Model\")\n",
    "            torch.save(\n",
    "                {\"model\": model.state_dict(), \"preds\": preds}, OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\"\n",
    "            )\n",
    "\n",
    "    check_point = torch.load(OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\")\n",
    "\n",
    "    valid_folds[\"preds\"] = check_point[\"preds\"]\n",
    "\n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d1d1b73-abf5-4f98-b9df-f68b0f9aa1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    predictions = []\n",
    "\n",
    "    test_dataset = BaseDataset(test, \"bert-base-uncased\", include_labels=False)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True\n",
    "    )\n",
    "\n",
    "    for fold in range(5):\n",
    "        LOGGER.info(f\"========== model: bert-base-uncased fold: {fold} inference ==========\")\n",
    "        model = BaseModel(\"bert-base-uncased\")\n",
    "        model.to(device)\n",
    "        if torch.cuda.device_count()>=2:\n",
    "            model = nn.DataParallel(model)\n",
    "        model.load_state_dict(torch.load(OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\")[\"model\"])\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        for i, (input_ids, attention_mask) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(input_ids, attention_mask)\n",
    "            preds.append(y_preds.to(\"cpu\").numpy())\n",
    "        preds = np.concatenate(preds)\n",
    "        predictions.append(preds)\n",
    "    predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a7303c8-8caa-4ca8-8824-450ecf406ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(result_df):\n",
    "    preds = result_df[\"preds\"].values\n",
    "    labels = result_df[\"judgement\"].values\n",
    "    score, border = metrics(labels, preds)\n",
    "    LOGGER.info(f\"Score: {score:<.5f}, Border: {border:<.5f}\")\n",
    "    return border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e7f1df6-599e-462f-87c9-320c5362a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if os.path.isfile(OUTPUT_DIR + \"oof_df.csv\"):\n",
    "        oof_df = pd.read_csv(OUTPUT_DIR + \"oof_df.csv\")\n",
    "        border = get_result(oof_df)\n",
    "    else:\n",
    "#     Training\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(5):\n",
    "            _oof_df = train_loop(train, fold)\n",
    "            oof_df = pd.concat([oof_df, _oof_df])\n",
    "            LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "            get_result(_oof_df)\n",
    "\n",
    "    # CV result\n",
    "    LOGGER.info(f\"========== CV ==========\")\n",
    "    border = get_result(oof_df)\n",
    "\n",
    "    # Save OOF result\n",
    "    oof_df.to_csv(OUTPUT_DIR + \"oof_df.csv\", index=False)\n",
    "    # Inference\n",
    "    predictions = inference()\n",
    "    predictions = np.where(predictions < border, 0, 1)\n",
    "\n",
    "    # submission\n",
    "    sub[\"judgement\"] = predictions\n",
    "    sub.to_csv(OUTPUT_DIR + \"submission.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4302906-8e71-4098-b3b0-592cf220063d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Score: 0.87696, Border: 0.20020\n",
      "========== CV ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bt:0.200202252200564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Score: 0.87696, Border: 0.20020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bt:0.200202252200564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: bert-base-uncased fold: 0 inference ==========\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e5d90aa6214192bac3b684bd8ba310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "========== model: bert-base-uncased fold: 1 inference ==========\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa095d9e98a45cd857ef417a36db988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "========== model: bert-base-uncased fold: 2 inference ==========\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb16b64-3f92-4f9f-aec2-a421f9b402a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2183653f-21ea-499f-846b-fa124ab4e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ps -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a097c0a6-783c-4f2f-8013-867f23f73ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86808b-88ff-4d4a-a356-addd86c93ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signate_env",
   "language": "python",
   "name": "signate_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
